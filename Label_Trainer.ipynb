{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e8720d",
   "metadata": {},
   "source": [
    "# Label-Agnostic Bayesian Optimization for IDS (Label Trainer)\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook tunes hyperparameters for an Intrusion Detection System (IDS) label trainer using **Bayesian Optimisation (BO)** and compares the result to a compact **1D-CNN** baseline. We optimise **PR-AUC** (Average Precision) via stratified cross-validation and then select a decision threshold $\\tau$ on the test set for **max-F1** or a **target Precision**.  \n",
    "\n",
    "Symbols and terms align with the Imperial **Professional Certificate in Machine Learning and AI** modules:\n",
    "- **Module 3:** Probabilistic modelling (Gaussian pdf/cdf), evaluation under imbalance (Precision–Recall curves).\n",
    "- **Module 10:** Model selection & black-box optimisation (hyperparameter tuning workflows, ensembles).\n",
    "- **Bandits/RL link:** Explore–exploit intuition for BO acquisition functions (see multi-armed bandit formulation).\n",
    "\n",
    "---\n",
    "\n",
    "### Why Hyperparameter Optimisation Matters\n",
    "We seek hyperparameters $\\theta$ (e.g., `learning_rate`, `num_leaves`, `max_depth`) that maximise a validation objective:\n",
    "\\[\n",
    "J(\\theta) = \\text{PR-AUC from stratified CV}.\n",
    "\\]\n",
    "\n",
    "In IDS:\n",
    "- Training is **expensive** (multiple folds).\n",
    "- Scores are **noisy** (variance across folds).\n",
    "- The function is **non-convex** (many local optima).  \n",
    "\n",
    "- **Grid search** is exhaustive but wasteful.  \n",
    "- **Random search** ignores prior trials.  \n",
    "- **Bayesian Optimisation** uses a surrogate + acquisition rule to prioritise *informative* trials, reaching good models in fewer iterations.\n",
    "\n",
    "---\n",
    "\n",
    "### Bayesian Optimisation: Concept\n",
    "Treat $J(\\theta)$ as a black-box function $f(\\theta)$. BO repeats:\n",
    "1. Evaluate initial $\\theta_i$, record $y_i=J(\\theta_i)$.\n",
    "2. Fit a **surrogate model** (often Gaussian Process) giving posterior mean $\\mu_t(\\theta)$ and uncertainty $\\sigma_t(\\theta)$.\n",
    "3. Choose next $\\theta$ by maximising an **acquisition function** $a_t(\\theta)$ that balances **exploration** (high $\\sigma_t$) and **exploitation** (high $\\mu_t$).\n",
    "4. Repeat until budget exhausted; return best $\\theta$.\n",
    "\n",
    "---\n",
    "\n",
    "### Surrogate Model (Gaussian Process, Module 3)\n",
    "Given $\\mathcal D_t=\\{(\\theta_i,y_i)\\}_{i=1}^t$ with $y_i=J(\\theta_i)+\\epsilon_i$, the GP posterior is:\n",
    "\\[\n",
    "$\\mu_t(\\theta) = k(\\theta,\\Theta_t)K_t^{-1}\\mathbf y_t, \\quad$ \n",
    "$\\sigma_t^2(\\theta) = k(\\theta,\\theta) - k(\\theta,\\Theta_t)K_t^{-1}k(\\Theta_t,\\theta)$.\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Acquisition Functions (Module 10, Bandits Explore–Exploit)\n",
    "Let $f^*=\\max_{i\\le t} y_i$. Using Gaussian $\\Phi,\\phi$:\n",
    "\n",
    "- **Expected Improvement (EI):**\n",
    "\\[\n",
    "$EI(\\theta) = (\\mu_t-f^*-\\xi)\\,\\Phi(z) + \\sigma_t\\,\\phi(z),\\quad z=\\frac{\\mu_t-f^*-\\xi}{\\sigma_t}$\n",
    "\\]\n",
    "- **Probability of Improvement (PI):**\n",
    "\\[\n",
    "$PI(\\theta)=\\Phi\\!\\left(\\frac{\\mu_t-f^*-\\xi}{\\sigma_t}\\right)$\n",
    "\\]\n",
    "- **Upper Confidence Bound (UCB):**\n",
    "\\[\n",
    "$UCB(\\theta)=\\mu_t + \\kappa\\,\\sigma_t$\n",
    "\\]\n",
    "\n",
    "Parameters $\\xi\\ge 0$ and $\\kappa>0$ control exploration vs exploitation — exactly the **multi-armed bandit** trade-off:contentReference[oaicite:3]{index=3}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879594f",
   "metadata": {},
   "source": [
    "## 2. Setup & Staging\n",
    "We configure global constants and disk **staging** helpers so every stage saves a `manifest.json` (plus artifacts). This lets us **resume** safely after any interruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd7af14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Setup & Staging\n",
    "import os, json, time, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_NAME = \"Label_Trainer\"\n",
    "PROJECT_VERSION = \"1.4\"\n",
    "RANDOM_STATE = 42\n",
    "N_THREADS = max(1, os.cpu_count() or 1)\n",
    "\n",
    "def stage_dir(name: str) -> Path:\n",
    "    d = Path(\"staging\") / name\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def save_stage(name: str, manifest: dict, **artifacts):\n",
    "    d = stage_dir(name)\n",
    "    (d / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "    for k, v in artifacts.items():\n",
    "        if hasattr(v, \"to_csv\"):\n",
    "            v.to_csv(d / f\"{k}.csv\", index=False)\n",
    "        else:\n",
    "            import joblib; joblib.dump(v, d / f\"{k}.joblib\")\n",
    "\n",
    "def load_stage(name: str):\n",
    "    d = Path(\"staging\") / name\n",
    "    m = d / \"manifest.json\"\n",
    "    return json.loads(m.read_text()) if m.exists() else None\n",
    "\n",
    "np.set_printoptions(suppress=True, linewidth=160)\n",
    "pd.set_option(\"display.width\", 160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c17ec",
   "metadata": {},
   "source": [
    "## 3. Data\n",
    "Load UNSW payloads and map the label to a binary target \\($y \\in \\{0,1\\}$\\) (1 = attack). Keep **X** as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bb327ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (79881, 1505) | Features: 1504\n",
      "Label source used: label (string→binary via benign≡0, else≡1)\n",
      "First 10 mapped labels: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 3) Data — robust binary label construction (handles strings like 'analysis')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"archive/Payload_data_UNSW.csv\")\n",
    "\n",
    "# Candidates in priority order\n",
    "CANDIDATES = [\"label\", \"label_str\", \"attack_cat\", \"class\", \"target\", \"y\"]\n",
    "present = [c for c in CANDIDATES if c in df.columns]\n",
    "assert present, f\"None of the expected label columns found. Present columns: {list(df.columns)[:20]}...\"\n",
    "\n",
    "label_src = present[0]  # pick first available\n",
    "series = df[label_src]\n",
    "\n",
    "def is_binary_numeric(s: pd.Series) -> bool:\n",
    "    # numeric and only {0,1} after dropping NaN\n",
    "    if not pd.api.types.is_numeric_dtype(s):\n",
    "        return False\n",
    "    vals = pd.unique(s.dropna())\n",
    "    return set(np.unique(vals)).issubset({0, 1})\n",
    "\n",
    "# Known benign tokens (lowercased)\n",
    "BENIGN_TOKENS = {\"benign\", \"normal\", \"background\", \"bg\", \"clean\"}\n",
    "# Sometimes datasets have a \"Label\" column with values {\"Attack\",\"Benign\"} or multiple attack categories.\n",
    "\n",
    "if is_binary_numeric(series):\n",
    "    y = series.astype(int).to_numpy()\n",
    "    src_used = label_src\n",
    "else:\n",
    "    # Treat any non-benign value as attack (1); benign tokens -> 0\n",
    "    s = series.astype(\"string\").str.strip().str.lower()\n",
    "    y = (~s.isin(BENIGN_TOKENS)).astype(int).to_numpy()\n",
    "    src_used = f\"{label_src} (string→binary via benign≡0, else≡1)\"\n",
    "\n",
    "# Sanity-check: ensure y ∈ {0,1}\n",
    "u = np.unique(y[~pd.isna(y)])\n",
    "assert set(u).issubset({0, 1}), f\"Label mapping failed; got values {u}. Inspect column '{label_src}' unique values: {series.unique()[:20]}\"\n",
    "\n",
    "# If we constructed from something other than 'label', ensure we keep an explicit numeric 'label' for consistency\n",
    "df[\"label\"] = y\n",
    "\n",
    "# Features X = all except 'label' (keep original text label columns around if you like; here we drop them for modeling)\n",
    "X = df.drop(columns=[\"label\"])\n",
    "y = df[\"label\"].values.astype(int)\n",
    "\n",
    "print(f\"Data shape: {df.shape} | Features: {X.shape[1]}\")\n",
    "print(f\"Label source used: {src_used}\")\n",
    "print(\"First 10 mapped labels:\", y[:10].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac93367",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split & Class Balance\n",
    "Use **stratified** split to preserve class ratio. Report positive rate, train/test counts. This checks leakage-free generalization setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d309944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (55916, 1504) | Test: (23965, 1504)\n",
      "Train positive rate: 0.7371\n",
      "Train dist: {1: 41216, 0: 14700}\n",
      "Test  dist: {1: 17665, 0: 6300}\n"
     ]
    }
   ],
   "source": [
    "# 4) Split & Class Balance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \"| Test:\", X_test.shape)\n",
    "print(f\"Train positive rate: {pd.Series(y_train).mean():.4f}\")\n",
    "print(\"Train dist:\", pd.Series(y_train).value_counts().to_dict())\n",
    "print(\"Test  dist:\", pd.Series(y_test).value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847318fe",
   "metadata": {},
   "source": [
    "## 5. Preprocess (categoricals/booleans)\n",
    "LightGBM needs numeric **or** pandas `category` dtypes. We:\n",
    "- convert `object → category` with aligned levels across train/test,\n",
    "- convert `bool → int8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c63348d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['protocol']\n"
     ]
    }
   ],
   "source": [
    "# Convert object→category with union of categories across train/test\n",
    "obj_cols_raw = [c for c in X_train.columns if X_train[c].dtype == \"object\"]\n",
    "for c in obj_cols_raw:\n",
    "    tr = X_train[c].astype(\"string\").fillna(\"__NA__\")\n",
    "    te = X_test[c].astype(\"string\").fillna(\"__NA__\")\n",
    "    cats = pd.Index(pd.concat([tr, te], axis=0).unique())\n",
    "    X_train[c] = pd.Categorical(tr, categories=cats)\n",
    "    X_test[c]  = pd.Categorical(te, categories=cats)\n",
    "\n",
    "# Booleans → ints\n",
    "for c in X_train.columns:\n",
    "    if X_train[c].dtype == bool:\n",
    "        X_train[c] = X_train[c].astype(np.int8)\n",
    "        X_test[c]  = X_test[c].astype(np.int8)\n",
    "\n",
    "# Record categorical columns\n",
    "cat_cols = [c for c in X_train.columns if str(X_train[c].dtype) == \"category\"]\n",
    "print(\"Categorical columns:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8559221",
   "metadata": {},
   "source": [
    "## 6. Metric Helpers\n",
    "Sweep thresholds \\( $\\tau$ \\) for **max-F1**; compute **TPR/TNR** at a chosen \\( $\\tau$ \\). We optimise **AUPRC** (PR-AUC) and *choose* \\( $\\tau$ \\) by F1 for operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d011fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    average_precision_score, f1_score, precision_recall_curve, confusion_matrix\n",
    ")\n",
    "\n",
    "def sweep_f1(y_true, p_hat, grid=np.linspace(0.05, 0.95, 19)):\n",
    "    f1s = []\n",
    "    for t in grid:\n",
    "        y_hat = (p_hat >= t).astype(int)\n",
    "        f1s.append(f1_score(y_true, y_hat, zero_division=0))\n",
    "    best = int(np.argmax(f1s))\n",
    "    return float(f1s[best]), float(grid[best])\n",
    "\n",
    "def tpr_tnr_at_tau(y_true, p_hat, tau: float):\n",
    "    y_hat = (p_hat >= tau).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else 0.0  # recall\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else 0.0  # specificity\n",
    "    return float(tpr), float(tnr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70343288",
   "metadata": {},
   "source": [
    "## 7. Baseline (LightGBM)\n",
    "Sanity-check the pipeline before expensive searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75635a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.44999999999999996 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# minimal check\n",
    "import numpy as np\n",
    "y_true = np.array([0,1,0,1,1,0,0,1])\n",
    "p_hat  = np.array([0.1,0.8,0.2,0.7,0.9,0.3,0.4,0.6])\n",
    "best_f1, tau = sweep_f1(y_true, p_hat)\n",
    "tpr, tnr = tpr_tnr_at_tau(y_true, p_hat, tau)\n",
    "print(best_f1, tau, tpr, tnr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fd534b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric helpers (patch) — safe to re-run anytime\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, f1_score, confusion_matrix\n",
    "\n",
    "def sweep_f1(y_true, p_hat, grid=np.linspace(0.05, 0.95, 19)):\n",
    "    \"\"\"\n",
    "    Return (best_f1, tau) by sweeping threshold tau over 'grid'.\n",
    "    Works for y_true in {0,1} and p_hat as probabilities in [0,1].\n",
    "    \"\"\"\n",
    "    f1s = []\n",
    "    for t in grid:\n",
    "        y_hat = (p_hat >= t).astype(int)\n",
    "        f1s.append(f1_score(y_true, y_hat, zero_division=0))\n",
    "    best = int(np.argmax(f1s)) if len(f1s) else 0\n",
    "    return float(f1s[best]), float(grid[best])\n",
    "\n",
    "def tpr_tnr_at_tau(y_true, p_hat, tau: float):\n",
    "    \"\"\"\n",
    "    Compute TPR (recall) and TNR (specificity) at decision threshold tau.\n",
    "    \"\"\"\n",
    "    y_hat = (p_hat >= tau).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else 0.0\n",
    "    return float(tpr), float(tnr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8dd1092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline AUPRC=0.9998 | F1@τ=0.9997 | τ=0.05 | TPR=1.000 | TNR=0.999\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, log_evaluation\n",
    "\n",
    "baseline = LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=250,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "baseline.fit(\n",
    "    X_train, y_train,\n",
    "    categorical_feature=cat_cols or None,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    callbacks=[log_evaluation(period=0)]\n",
    ")\n",
    "\n",
    "p_hat_base = baseline.predict_proba(X_test)[:, 1]\n",
    "auprc_base = average_precision_score(y_test, p_hat_base)\n",
    "f1_base, tau_base = sweep_f1(y_test, p_hat_base)\n",
    "tpr_base, tnr_base = tpr_tnr_at_tau(y_test, p_hat_base, tau_base)\n",
    "\n",
    "print(f\"Baseline AUPRC={auprc_base:.4f} | F1@τ={f1_base:.4f} | τ={tau_base:.2f} | TPR={tpr_base:.3f} | TNR={tnr_base:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72004b",
   "metadata": {},
   "source": [
    "## 8. Manual Grid (Parallel + Resume)\n",
    "Search `(num_leaves, learning_rate)` with `joblib.Parallel`, **checkpoint** results to CSV, and **skip** completed configs on rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d873c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sh/njlwqz496112jgym_0_p82h80000gn/T/ipykernel_50502/3808631.py:57: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_leaves",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "AUPRC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tau",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "167d31aa-689b-4d8f-b40f-f05dbb99dbde",
       "rows": [
        [
         "0",
         "31",
         "0.05",
         "0.9997634333419292",
         "0.9997028988582828",
         "0.05"
        ],
        [
         "1",
         "127",
         "0.05",
         "0.9997579591042078",
         "0.999660441426146",
         "0.05"
        ],
        [
         "2",
         "63",
         "0.05",
         "0.999743141360328",
         "0.999660441426146",
         "0.05"
        ],
        [
         "3",
         "63",
         "0.1",
         "0.999711571814283",
         "0.9997028988582828",
         "0.05"
        ],
        [
         "4",
         "31",
         "0.1",
         "0.9997017739595192",
         "0.9997028988582828",
         "0.05"
        ],
        [
         "5",
         "127",
         "0.2",
         "0.9996776836854973",
         "0.999660441426146",
         "0.05"
        ],
        [
         "6",
         "127",
         "0.1",
         "0.9996657276996088",
         "0.999660441426146",
         "0.05"
        ],
        [
         "7",
         "63",
         "0.2",
         "0.9996021050534228",
         "0.999660441426146",
         "0.05"
        ],
        [
         "8",
         "31",
         "0.2",
         "0.999592059127384",
         "0.9997028988582828",
         "0.05"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>AUPRC</th>\n",
       "      <th>F1</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.999763</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>0.999660</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.999743</td>\n",
       "      <td>0.999660</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>127</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.999678</td>\n",
       "      <td>0.999660</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>127</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.999660</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.999602</td>\n",
       "      <td>0.999660</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.999592</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_leaves  learning_rate     AUPRC        F1   tau\n",
       "0          31           0.05  0.999763  0.999703  0.05\n",
       "1         127           0.05  0.999758  0.999660  0.05\n",
       "2          63           0.05  0.999743  0.999660  0.05\n",
       "3          63           0.10  0.999712  0.999703  0.05\n",
       "4          31           0.10  0.999702  0.999703  0.05\n",
       "5         127           0.20  0.999678  0.999660  0.05\n",
       "6         127           0.10  0.999666  0.999660  0.05\n",
       "7          63           0.20  0.999602  0.999660  0.05\n",
       "8          31           0.20  0.999592  0.999703  0.05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "grid = {\n",
    "    \"num_leaves\": [31, 63, 127],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "def eval_cfg(nl, lr):\n",
    "    from lightgbm import LGBMClassifier, log_evaluation\n",
    "    clf = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        n_estimators=500,\n",
    "        num_leaves=nl,\n",
    "        learning_rate=lr,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\",\n",
    "        verbosity=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train, categorical_feature=cat_cols or None, callbacks=[log_evaluation(period=0)])\n",
    "    p_hat = clf.predict_proba(X_test)[:, 1]\n",
    "    auprc = average_precision_score(y_test, p_hat)\n",
    "    f1, tau = sweep_f1(y_test, p_hat)\n",
    "    return {\"num_leaves\": nl, \"learning_rate\": lr, \"AUPRC\": auprc, \"F1\": f1, \"tau\": tau}\n",
    "\n",
    "mg_dir = stage_dir(\"manual_grid\")\n",
    "res_path = mg_dir / \"results.csv\"\n",
    "prev = pd.read_csv(res_path) if res_path.exists() else pd.DataFrame()\n",
    "done = set(zip(prev.get(\"num_leaves\", []), prev.get(\"learning_rate\", [])))\n",
    "\n",
    "candidates = [(nl, lr) for nl in grid[\"num_leaves\"] for lr in grid[\"learning_rate\"] if (nl, lr) not in done]\n",
    "if candidates:\n",
    "    rows = Parallel(n_jobs=min(N_THREADS, len(candidates)))(\n",
    "        delayed(eval_cfg)(nl, lr) for nl, lr in candidates\n",
    "    )\n",
    "    results = pd.concat([prev, pd.DataFrame(rows)], ignore_index=True)\n",
    "else:\n",
    "    results = prev\n",
    "\n",
    "if not results.empty:\n",
    "    results = results.sort_values([\"AUPRC\", \"F1\"], ascending=[False, False]).reset_index(drop=True)\n",
    "    results.to_csv(res_path, index=False)\n",
    "    best = results.iloc[0].to_dict()\n",
    "    # retrain best to persist model & compute TPR/TNR at tau\n",
    "    from lightgbm import LGBMClassifier, log_evaluation\n",
    "    best_clf = LGBMClassifier(\n",
    "        objective=\"binary\", n_estimators=500,\n",
    "        num_leaves=int(best[\"num_leaves\"]), learning_rate=float(best[\"learning_rate\"]),\n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE, n_jobs=-1,\n",
    "        class_weight=\"balanced\", verbosity=-1\n",
    "    ).fit(X_train, y_train, categorical_feature=cat_cols or None, callbacks=[log_evaluation(period=0)])\n",
    "    p_best = best_clf.predict_proba(X_test)[:, 1]\n",
    "    tpr, tnr = tpr_tnr_at_tau(y_test, p_best, float(best[\"tau\"]))\n",
    "    save_stage(\"manual_grid\", {\n",
    "        \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n",
    "        \"stage\": \"manual_grid\",\n",
    "        \"model_name\": \"LightGBM\",\n",
    "        \"dataset\": \"archive/Payload_data_UNSW.csv\",\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"metrics\": {\"AUPRC\": float(best[\"AUPRC\"]), \"F1\": float(best[\"F1\"]), \"tau\": float(best[\"tau\"]),\n",
    "                    \"TPR\": tpr, \"TNR\": tnr},\n",
    "        \"params\": {\"num_leaves\": int(best[\"num_leaves\"]), \"learning_rate\": float(best[\"learning_rate\"])}\n",
    "    }, results=results, model=best_clf)\n",
    "\n",
    "display(results.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd0108",
   "metadata": {},
   "source": [
    "## 8b. Automated HPO — Randomized Search (Stratified CV, resumable)\n",
    "We use `RandomizedSearchCV` to explore a larger space quickly. Objective = **PR-AUC** (Average Precision).\n",
    "- **Resume-safe:** results saved to `staging/random_search/` (CSV + manifest).\n",
    "- **Fit-time cat handling:** pass `categorical_feature=cat_cols` to LightGBM during `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ace9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    }
   ],
   "source": [
    "# 8b) Automated HPO — Randomized Search (resumable)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import randint, loguniform\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "rs_dir = stage_dir(\"random_search\")\n",
    "rs_path = rs_dir / \"results.csv\"\n",
    "\n",
    "param_distributions = {\n",
    "    \"num_leaves\": randint(16, 256),\n",
    "    \"max_depth\": randint(2, 16),\n",
    "    \"min_child_samples\": randint(10, 200),\n",
    "    \"subsample\": loguniform(0.6, 1.0),          # sampled in (0.6, 1.0]\n",
    "    \"colsample_bytree\": loguniform(0.6, 1.0),\n",
    "    \"learning_rate\": loguniform(1e-3, 2e-1),\n",
    "    \"reg_lambda\": loguniform(1e-3, 10.0),\n",
    "}\n",
    "\n",
    "rs_base = LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=500,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=rs_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=40,\n",
    "    scoring=\"average_precision\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=1,\n",
    "    refit=True  # refit best on full training folds\n",
    ")\n",
    "\n",
    "# pass categorical_feature to the underlying estimator during fit\n",
    "fit_kwargs = {\"categorical_feature\": cat_cols or None}\n",
    "rs.fit(X_train, y_train, **fit_kwargs)\n",
    "\n",
    "best_rs = rs.best_estimator_\n",
    "p_rs = best_rs.predict_proba(X_test)[:, 1]\n",
    "auprc_rs = average_precision_score(y_test, p_rs)\n",
    "f1_rs, tau_rs = sweep_f1(y_test, p_rs)\n",
    "tpr_rs, tnr_rs = tpr_tnr_at_tau(y_test, p_rs, tau_rs)\n",
    "\n",
    "# Persist result table (cv_results_) and manifest\n",
    "res_df = pd.DataFrame(rs.cv_results_)\n",
    "res_df.to_csv(rs_path, index=False)\n",
    "\n",
    "save_stage(\"random_search\", {\n",
    "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"stage\": \"random_search\",\n",
    "    \"model_name\": \"LightGBM_RS\",\n",
    "    \"dataset\": \"archive/Payload_data_UNSW.csv\",\n",
    "    \"seed\": RANDOM_STATE,\n",
    "    \"metrics\": {\"AUPRC\": float(auprc_rs), \"F1\": float(f1_rs), \"tau\": float(tau_rs),\n",
    "                \"TPR\": float(tpr_rs), \"TNR\": float(tnr_rs)},\n",
    "    \"best_params\": rs.best_params_\n",
    "}, results=res_df, model=best_rs)\n",
    "\n",
    "print(\"Randomized Search | AUPRC={:.4f} F1@τ={:.4f} τ={:.2f}\".format(auprc_rs, f1_rs, tau_rs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d791bf",
   "metadata": {},
   "source": [
    "## 9. Bayesian Optimisation (Checkpoint & Resume)\n",
    "`BayesSearchCV` + `CheckpointSaver`. Objective = **PR-AUC**. After BO, select **τ** by max-F1 on test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b90a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "from skopt.callbacks import CheckpointSaver\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "bo_dir = stage_dir('bo_lgb')\n",
    "ckpt = bo_dir / 'skopt_ckpt.pkl'\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    objective='binary',\n",
    "    n_estimators=500,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "search_spaces = {\n",
    "    'num_leaves': Integer(31, 255),\n",
    "    'max_depth': Integer(2, 16),\n",
    "    'min_child_samples': Integer(10, 200),\n",
    "    'subsample': Real(0.6, 1.0),\n",
    "    'colsample_bytree': Real(0.6, 1.0),\n",
    "    'learning_rate': Real(1e-3, 2e-1, prior='log-uniform'),\n",
    "    'reg_lambda': Real(1e-3, 10.0, prior='log-uniform')\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "callbacks = [CheckpointSaver(str(ckpt), compress=9)]  # correct signature\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    estimator=lgb,\n",
    "    search_spaces=search_spaces,\n",
    "    n_iter=40,\n",
    "    scoring='average_precision',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train, callback=callbacks)\n",
    "\n",
    "print('Best params:', opt.best_params_)\n",
    "print(f'Best CV PR-AUC: {opt.best_score_:.4f}')\n",
    "lgb_bo = opt.best_estimator_\n",
    "\n",
    "y_pred_proba_bo = lgb_bo.predict_proba(X_test)[:, 1]\n",
    "auprc_bo = average_precision_score(y_test, y_pred_proba_bo)\n",
    "f1_bo, tau_bo = sweep_f1(y_test, y_pred_proba_bo)\n",
    "tpr_bo, tnr_bo = tpr_tnr_at_tau(y_test, y_pred_proba_bo, tau_bo)\n",
    "\n",
    "save_stage('bo_lgb', {\n",
    "    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n",
    "    'stage': 'bo_lgb',\n",
    "    'model_name': 'LightGBM_BO',\n",
    "    'dataset': 'archive/Payload_data_UNSW.csv',\n",
    "    'seed': RANDOM_STATE,\n",
    "    'metrics': {'AUPRC': float(auprc_bo), 'F1': float(f1_bo), 'tau': float(tau_bo),\n",
    "                'TPR': float(tpr_bo), 'TNR': float(tnr_bo)},\n",
    "    'best_params': opt.best_params_\n",
    "}, model=lgb_bo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd5cc32",
   "metadata": {},
   "source": [
    "## 9b. Enhanced BO — ask–tell (warm start, batch EI, patience, resume)\n",
    "We switch to `skopt.Optimizer` for fine control:\n",
    "- **Warm start** from Manual Grid + Randomized Search winners\n",
    "- **Batch suggestions** (`n_points`) for multi-core eval\n",
    "- **Patience** (early-stop if no PR-AUC improvement)\n",
    "- **Resume** from `staging/bo_enhanced/optimizer.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847100d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9b) Enhanced BO — ask–tell (warm start, batch, patience, resume)\n",
    "\n",
    "from skopt import Optimizer\n",
    "from skopt.space import Integer, Real\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "ben_dir = stage_dir(\"bo_enhanced\")\n",
    "opt_ckpt = ben_dir / \"optimizer.pkl\"\n",
    "res_csv  = ben_dir / \"results.csv\"\n",
    "\n",
    "# Search space\n",
    "space = [\n",
    "    Integer(31, 255, name=\"num_leaves\"),\n",
    "    Integer(2, 16, name=\"max_depth\"),\n",
    "    Integer(10, 200, name=\"min_child_samples\"),\n",
    "    Real(0.6, 1.0, name=\"subsample\"),\n",
    "    Real(0.6, 1.0, name=\"colsample_bytree\"),\n",
    "    Real(1e-3, 2e-1, prior=\"log-uniform\", name=\"learning_rate\"),\n",
    "    Real(1e-3, 10.0, prior=\"log-uniform\", name=\"reg_lambda\"),\n",
    "]\n",
    "\n",
    "# Build warm-start points from staged results if available\n",
    "X0, y0 = [], []\n",
    "for stage in [\"manual_grid\", \"random_search\", \"bo_lgb\"]:\n",
    "    m = load_stage(stage)\n",
    "    if m and \"params\" in m:\n",
    "        p = m[\"params\"]\n",
    "        if stage == \"bo_lgb\" and \"best_params\" in m:\n",
    "            p = m[\"best_params\"]\n",
    "        x = [int(p.get(\"num_leaves\", 63)),\n",
    "             int(p.get(\"max_depth\", 6)),\n",
    "             int(p.get(\"min_child_samples\", 20)),\n",
    "             float(p.get(\"subsample\", 0.8)),\n",
    "             float(p.get(\"colsample_bytree\", 0.8)),\n",
    "             float(p.get(\"learning_rate\", 0.1)),\n",
    "             float(p.get(\"reg_lambda\", 1.0))]\n",
    "        # If metrics present, use negative AUPRC (Optimizer minimizes)\n",
    "        ap = m[\"metrics\"].get(\"AUPRC\") if \"metrics\" in m else None\n",
    "        if ap is not None:\n",
    "            X0.append(x); y0.append(-float(ap))\n",
    "\n",
    "# Initialize or resume Optimizer\n",
    "if opt_ckpt.exists():\n",
    "    import joblib\n",
    "    opt = joblib.load(opt_ckpt)\n",
    "else:\n",
    "    opt = Optimizer(dimensions=space, base_estimator=\"GP\", acq_func=\"EI\", random_state=RANDOM_STATE)\n",
    "    if X0:\n",
    "        opt.tell(X0, y0)  # warm-start\n",
    "\n",
    "# CV evaluator (3-fold AP) with LightGBM + cat cols\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def ap_cv(params_dict):\n",
    "    clf = LGBMClassifier(\n",
    "        objective=\"binary\", n_estimators=500, random_state=RANDOM_STATE,\n",
    "        n_jobs=-1, class_weight=\"balanced\", verbosity=-1, **params_dict\n",
    "    )\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    ap_scores = []\n",
    "    for tr_idx, va_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
    "        y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n",
    "        clf.fit(X_tr, y_tr, categorical_feature=cat_cols or None)\n",
    "        p = clf.predict_proba(X_va)[:, 1]\n",
    "        ap_scores.append(average_precision_score(y_va, p))\n",
    "    return float(np.mean(ap_scores))\n",
    "\n",
    "# Ask–tell loop (batch), with patience & checkpoint\n",
    "N_ITERS   = 30\n",
    "BATCH     = min(4, N_THREADS)  # parallel batch size\n",
    "PATIENCE  = 6\n",
    "best_ap   = max([-y for y in y0], default=-np.inf)\n",
    "no_improve= 0\n",
    "\n",
    "# Load prior CSV if exists\n",
    "hist = pd.read_csv(res_csv) if res_csv.exists() else pd.DataFrame(columns=[\n",
    "    \"num_leaves\",\"max_depth\",\"min_child_samples\",\"subsample\",\"colsample_bytree\",\"learning_rate\",\"reg_lambda\",\"AP_cv\"\n",
    "])\n",
    "\n",
    "for step in range(N_ITERS):\n",
    "    X_batch = opt.ask(n_points=BATCH)\n",
    "    # Convert list→dict for evaluator\n",
    "    def vec_to_params(v):\n",
    "        return {\n",
    "            \"num_leaves\": int(v[0]), \"max_depth\": int(v[1]), \"min_child_samples\": int(v[2]),\n",
    "            \"subsample\": float(v[3]), \"colsample_bytree\": float(v[4]),\n",
    "            \"learning_rate\": float(v[5]), \"reg_lambda\": float(v[6])\n",
    "        }\n",
    "\n",
    "    params_batch = [vec_to_params(v) for v in X_batch]\n",
    "    scores = Parallel(n_jobs=BATCH)(\n",
    "        delayed(ap_cv)(p) for p in params_batch\n",
    "    )\n",
    "    # Optimizer minimizes, so pass negative AP\n",
    "    opt.tell(X_batch, [-s for s in scores])\n",
    "\n",
    "    # Update history\n",
    "    rows = [dict(**p, AP_cv=s) for p, s in zip(params_batch, scores)]\n",
    "    hist = pd.concat([hist, pd.DataFrame(rows)], ignore_index=True)\n",
    "    hist.to_csv(res_csv, index=False)\n",
    "\n",
    "    # Early stopping on AP\n",
    "    batch_best = max(scores)\n",
    "    if batch_best > best_ap + 1e-6:\n",
    "        best_ap = batch_best\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    # Checkpoint optimizer\n",
    "    import joblib\n",
    "    joblib.dump(opt, opt_ckpt)\n",
    "\n",
    "    print(f\"[Enhanced BO] step {step+1}/{N_ITERS} | best_cv_AP={best_ap:.4f} | patience={no_improve}/{PATIENCE}\")\n",
    "    if no_improve >= PATIENCE:\n",
    "        print(\"Early stopping (patience reached).\")\n",
    "        break\n",
    "\n",
    "# Select best params from history, retrain on full train, evaluate on test\n",
    "if not hist.empty:\n",
    "    best_row = hist.sort_values(\"AP_cv\", ascending=False).iloc[0].to_dict()\n",
    "    best_params_enh = {k: best_row[k] for k in [\"num_leaves\",\"max_depth\",\"min_child_samples\",\"subsample\",\"colsample_bytree\",\"learning_rate\",\"reg_lambda\"]}\n",
    "    best_enh = LGBMClassifier(\n",
    "        objective=\"binary\", n_estimators=500, random_state=RANDOM_STATE, n_jobs=-1,\n",
    "        class_weight=\"balanced\", verbosity=-1, **best_params_enh\n",
    "    ).fit(X_train, y_train, categorical_feature=cat_cols or None)\n",
    "\n",
    "    p_enh = best_enh.predict_proba(X_test)[:, 1]\n",
    "    auprc_enh = average_precision_score(y_test, p_enh)\n",
    "    f1_enh, tau_enh = sweep_f1(y_test, p_enh)\n",
    "    tpr_enh, tnr_enh = tpr_tnr_at_tau(y_test, p_enh, tau_enh)\n",
    "\n",
    "    save_stage(\"bo_enhanced\", {\n",
    "        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"stage\": \"bo_enhanced\",\n",
    "        \"model_name\": \"LightGBM_BO_Enhanced\",\n",
    "        \"dataset\": \"archive/Payload_data_UNSW.csv\",\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"metrics\": {\"AUPRC\": float(auprc_enh), \"F1\": float(f1_enh), \"tau\": float(tau_enh),\n",
    "                    \"TPR\": float(tpr_enh), \"TNR\": float(tnr_enh)},\n",
    "        \"params\": best_params_enh\n",
    "    }, results=hist, model=best_enh)\n",
    "\n",
    "    print(\"Enhanced BO | AUPRC={:.4f} F1@τ={:.4f} τ={:.2f}\".format(auprc_enh, f1_enh, tau_enh))\n",
    "else:\n",
    "    print(\"Enhanced BO produced no evaluations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e656fd",
   "metadata": {},
   "source": [
    "## 10. Champion Selection\n",
    "Compare **Manual Grid** vs **BO** by AUPRC then F1. Declare the **champion**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562488c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Champion Selection — now includes Random Search and Enhanced BO\n",
    "\n",
    "metrics = {}\n",
    "for stage in [\"manual_grid\", \"random_search\", \"bo_lgb\", \"bo_enhanced\"]:\n",
    "    m = load_stage(stage)\n",
    "    if m and \"metrics\" in m:\n",
    "        metrics[stage] = m[\"metrics\"]\n",
    "\n",
    "if metrics:\n",
    "    dfm = pd.DataFrame(metrics).T\n",
    "    display(dfm)\n",
    "    champ_name = dfm.sort_values([\"AUPRC\", \"F1\"], ascending=[False, False]).index[0]\n",
    "    champ = dfm.loc[champ_name].to_dict()\n",
    "    print(\"Champion:\", champ_name, \"|\", champ)\n",
    "else:\n",
    "    champ_name, champ = None, None\n",
    "    print(\"No metrics found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa03805",
   "metadata": {},
   "source": [
    "## 11. SOC Report (TPR/TNR @ τ) + Confusion Matrix\n",
    "Operational view for detection coverage (**TPR**) vs false-positives (**TNR**) at the selected \\( \\tau \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def soc_report(y_true, p_hat, tau):\n",
    "    y_hat = (p_hat >= tau).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else 0.0\n",
    "    print(f\"τ={tau:.2f} | TPR={tpr:.3f} | TNR={tnr:.3f} | TP={tp} FP={fp} TN={tn} FN={fn}\")\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_hat)\n",
    "    plt.title(f\"Confusion Matrix @ τ={tau:.2f}\")\n",
    "    plt.show()\n",
    "\n",
    "# Prefer BO if available\n",
    "if bo:\n",
    "    soc_report(y_test, y_pred_proba_bo, tau_bo)\n",
    "elif mg:\n",
    "    # fallback to baseline if needed\n",
    "    soc_report(y_test, p_hat_base, tau_base)\n",
    "else:\n",
    "    soc_report(y_test, p_hat_base, tau_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0586882",
   "metadata": {},
   "source": [
    "## 12. Auto-Wire Docs\n",
    "Inject latest **AUPRC / F1 / τ** into `model_card.md` and `README.md` between `<!--METRICS_START--> … <!--METRICS_END-->` (append if absent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "manifest = load_stage(\"bo_lgb\") or load_stage(\"manual_grid\")\n",
    "if manifest:\n",
    "    m = manifest[\"metrics\"]\n",
    "    auprc = m.get(\"AUPRC\") or m.get(\"auprc\")\n",
    "    f1 = m.get(\"F1\") or m.get(\"f1\")\n",
    "    tau = m.get(\"tau\")\n",
    "    block = (\n",
    "        \"<!--METRICS_START-->\\n\"\n",
    "        f\"AUPRC: {auprc:.4f}\\n\"\n",
    "        f\"F1: {f1:.4f}\\n\"\n",
    "        f\"τ: {tau:.2f}\\n\"\n",
    "        \"<!--METRICS_END-->\\n\"\n",
    "    )\n",
    "    for doc in [\"model_card.md\", \"README.md\"]:\n",
    "        p = Path(doc)\n",
    "        if not p.exists(): \n",
    "            continue\n",
    "        txt = p.read_text()\n",
    "        if \"<!--METRICS_START-->\" in txt and \"<!--METRICS_END-->\" in txt:\n",
    "            pre, _, rest = txt.partition(\"<!--METRICS_START-->\")\n",
    "            _, _, post = rest.partition(\"<!--METRICS_END-->\")\n",
    "            txt = pre + block + post\n",
    "        else:\n",
    "            if not txt.endswith(\"\\n\"):\n",
    "                txt += \"\\n\"\n",
    "            txt += \"\\n\" + block\n",
    "        p.write_text(txt)\n",
    "    print(\"Docs updated.\")\n",
    "else:\n",
    "    print(\"No manifest available to auto-wire docs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013607f",
   "metadata": {},
   "source": [
    "## 13. Version Log\n",
    "Append a versioned entry to the changelog for auditability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = Path(\"CHANGELOG_Label_Trainer.md\")\n",
    "entry = f\"- {datetime.utcnow().isoformat()}Z | v{PROJECT_VERSION} | v1.4 pipeline: staging/resume, manual grid, BO checkpoint, SOC report, auto-wire\\n\"\n",
    "if log.exists():\n",
    "    prev = log.read_text()\n",
    "    if entry not in prev:\n",
    "        log.write_text(prev + entry)\n",
    "else:\n",
    "    log.write_text(entry)\n",
    "print(\"CHANGELOG updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6196a44d",
   "metadata": {},
   "source": [
    "## Tools for justify BO & hyperparams\n",
    "\n",
    "* Tables (Cell 14): compact, reproducible evidence across all methods.\n",
    "\n",
    "* Convergence (Cell 15): shows best-so-far CV AP improving as BO progresses.\n",
    "\n",
    "* Response plots (Cell 16): illustrates sensitivity of AP vs hyperparameters and supports the selected values.\n",
    "\n",
    "* PR curves (Cell 17): operationally relevant under class imbalance (AUPRC focus).\n",
    "\n",
    "* F1 vs τ (Cell 18): makes threshold choice transparent; useful for SOC policy.\n",
    "\n",
    "* Feature importance (Cell 19): supports LightGBM interpretability claims.\n",
    "\n",
    "* CNN benchmark (Cell 20): fair comparison if you have a 1D-CNN baseline; otherwise it’s a clean slot to drop results into.\n",
    "\n",
    "* Champion auto-wire (Cell 21): pushes the winner into your docs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3683bd",
   "metadata": {},
   "source": [
    "## 14. Aggregate Metrics Table (Manual, Random, BO, Enhanced BO, CNN)\n",
    "Summarise **AUPRC**, **F1**, **τ**, **TPR**, **TNR** for each stage. If a 1D-CNN benchmark is staged under `staging/cnn1d/manifest.json`, it is included automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79418ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Aggregate metrics table\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "stages = [\"manual_grid\", \"random_search\", \"bo_lgb\", \"bo_enhanced\", \"cnn1d\"]\n",
    "rows = []\n",
    "for s in stages:\n",
    "    m = load_stage(s)\n",
    "    if not m or \"metrics\" not in m:\n",
    "        continue\n",
    "    r = {\"stage\": s}\n",
    "    r.update({k: m[\"metrics\"].get(k) for k in [\"AUPRC\", \"F1\", \"tau\", \"TPR\", \"TNR\"]})\n",
    "    rows.append(r)\n",
    "\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "if not metrics_df.empty:\n",
    "    display(metrics_df.sort_values([\"AUPRC\",\"F1\"], ascending=[False, False]).reset_index(drop=True))\n",
    "else:\n",
    "    print(\"No staged metrics found. Run training and HPO first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ee3e6",
   "metadata": {},
   "source": [
    "## 15. BO Convergence\n",
    "Show optimisation progress:\n",
    "- **BayesSearchCV**: proxy best-so-far from CV (if history available).\n",
    "- **Enhanced BO**: `AP_cv` per step from `staging/bo_enhanced/results.csv`.\n",
    "This justifies BO’s **sample efficiency** vs manual/random search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) BO convergence plots (matplotlib only)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enhanced BO history\n",
    "ben_dir = Path(\"staging/bo_enhanced\")\n",
    "ben_hist = ben_dir / \"results.csv\"\n",
    "\n",
    "plt.figure()\n",
    "if ben_hist.exists():\n",
    "    h = pd.read_csv(ben_hist)\n",
    "    if not h.empty and \"AP_cv\" in h.columns:\n",
    "        best_so_far = h[\"AP_cv\"].cummax().values\n",
    "        plt.plot(range(1, len(best_so_far)+1), best_so_far, marker='o', linewidth=1)\n",
    "        plt.xlabel(\"Enhanced BO evaluations\")\n",
    "        plt.ylabel(\"Best CV AP so far\")\n",
    "        plt.title(\"Enhanced BO Convergence (best-so-far CV AP)\")\n",
    "        plt.grid(True, linewidth=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"No AP_cv history yet.\", ha='center')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, \"staging/bo_enhanced/results.csv not found.\", ha='center')\n",
    "plt.show()\n",
    "\n",
    "# BayesSearchCV does not expose per-iteration history portably; show final best if staged\n",
    "bo = load_stage(\"bo_lgb\")\n",
    "if bo and \"metrics\" in bo:\n",
    "    print(\"BayesSearchCV best (test): AUPRC={:.4f}, F1@τ={:.4f}, τ={:.2f}\".format(\n",
    "        bo[\"metrics\"].get(\"AUPRC\", float('nan')),\n",
    "        bo[\"metrics\"].get(\"F1\", float('nan')),\n",
    "        bo[\"metrics\"].get(\"tau\", float('nan')),\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd4f81e",
   "metadata": {},
   "source": [
    "## 16. Hyperparameter Response (From Enhanced BO History)\n",
    "Visual diagnostics of **hyperparameters vs CV AP** (scatter) + a **parallel coordinates-style** view (numeric columns). These help justify the **chosen hyperparameters** and show sensitivity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) Hyperparameter response plots from enhanced BO history\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "ben_hist = Path(\"staging/bo_enhanced/results.csv\")\n",
    "if ben_hist.exists():\n",
    "    h = pd.read_csv(ben_hist)\n",
    "    if not h.empty and \"AP_cv\" in h.columns:\n",
    "        # Scatter: each hyperparam vs AP\n",
    "        hp_cols = [\"num_leaves\",\"max_depth\",\"min_child_samples\",\"subsample\",\"colsample_bytree\",\"learning_rate\",\"reg_lambda\"]\n",
    "        for c in hp_cols:\n",
    "            if c in h.columns:\n",
    "                plt.figure()\n",
    "                plt.scatter(h[c].values, h[\"AP_cv\"].values, s=12)\n",
    "                plt.xlabel(c)\n",
    "                plt.ylabel(\"CV AP\")\n",
    "                plt.title(f\"Hyperparameter response: {c} vs CV AP\")\n",
    "                plt.grid(True, linewidth=0.3)\n",
    "                plt.show()\n",
    "\n",
    "        # Parallel-coordinates-style view (bucket top/bottom)\n",
    "        try:\n",
    "            tmp = h.copy()\n",
    "            # Bucket AP into categories to plot\n",
    "            q = tmp[\"AP_cv\"].quantile([0.25, 0.5, 0.75]).to_dict()\n",
    "            def bucket(x):\n",
    "                if x >= q[0.75]: return \"top\"\n",
    "                if x <= q[0.25]: return \"bottom\"\n",
    "                return \"mid\"\n",
    "            tmp[\"bucket\"] = tmp[\"AP_cv\"].apply(bucket)\n",
    "            pc_cols = [c for c in hp_cols if c in tmp.columns]\n",
    "            if pc_cols:\n",
    "                pc_df = tmp[pc_cols + [\"bucket\"]].copy()\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                # fallback simple parallel plot: normalize columns\n",
    "                norm = pc_df[pc_cols].astype(float)\n",
    "                norm = (norm - norm.min()) / (norm.max() - norm.min() + 1e-12)\n",
    "                norm[\"bucket\"] = pc_df[\"bucket\"]\n",
    "                # emulate parallel_coordinates (without color control)\n",
    "                for label in [\"bottom\",\"mid\",\"top\"]:\n",
    "                    sub = norm[norm[\"bucket\"] == label]\n",
    "                    if sub.empty: \n",
    "                        continue\n",
    "                    for _, row in sub.iterrows():\n",
    "                        plt.plot(range(len(pc_cols)), row[pc_cols].values, alpha=0.3)\n",
    "                plt.xticks(range(len(pc_cols)), pc_cols, rotation=45, ha=\"right\")\n",
    "                plt.ylabel(\"Normalized hyperparameter value\")\n",
    "                plt.title(\"Parallel view of top/mid/bottom AP configurations\")\n",
    "                plt.grid(True, linewidth=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        except Exception as e:\n",
    "            print(\"Parallel view failed:\", e)\n",
    "    else:\n",
    "        print(\"Enhanced BO history has no AP_cv column.\")\n",
    "else:\n",
    "    print(\"Enhanced BO history not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760c380",
   "metadata": {},
   "source": [
    "## 17. Precision–Recall Curves (Champion vs Baseline vs 1D-CNN)\n",
    "Compare **PR curves** to justify BO’s gains where it matters (high precision at actionable recall).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) PR Curves for comparison: champion vs baseline vs CNN (if available)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Helper to compute PR arrays from proba\n",
    "def pr_arrays(y_true, p_hat):\n",
    "    p, r, t = precision_recall_curve(y_true, p_hat)\n",
    "    ap = average_precision_score(y_true, p_hat)\n",
    "    return p, r, ap\n",
    "\n",
    "# Champion = prefer enhanced BO, then BO, then random, then manual, else baseline\n",
    "p_preds = {}\n",
    "labels = {}\n",
    "\n",
    "# baseline\n",
    "p_preds[\"baseline\"] = p_hat_base\n",
    "labels[\"baseline\"] = (\"Baseline\", average_precision_score(y_test, p_hat_base))\n",
    "\n",
    "# staged champions\n",
    "order = [\"bo_enhanced\",\"bo_lgb\",\"random_search\",\"manual_grid\"]\n",
    "for s in order:\n",
    "    m = load_stage(s)\n",
    "    if not m: \n",
    "        continue\n",
    "    # Attempt to re-create proba if model saved; else skip curve but show AP from manifest\n",
    "    model_path = Path(\"staging\")/s/\"model.joblib\"\n",
    "    if model_path.exists():\n",
    "        import joblib\n",
    "        mdl = joblib.load(model_path)\n",
    "        p_preds[s] = mdl.predict_proba(X_test)[:,1]\n",
    "        labels[s] = (s, average_precision_score(y_test, p_preds[s]))\n",
    "\n",
    "# CNN (if staged)\n",
    "cnn = load_stage(\"cnn1d\")\n",
    "if cnn:\n",
    "    cnn_model = Path(\"staging\")/\"cnn1d\"/\"model.joblib\"\n",
    "    if cnn_model.exists():\n",
    "        import joblib\n",
    "        mdl = joblib.load(cnn_model)\n",
    "        # Expect a predict_proba-like method or predict giving probabilities\n",
    "        try:\n",
    "            p_preds[\"cnn1d\"] = mdl.predict_proba(X_test)[:, 1]\n",
    "        except Exception:\n",
    "            # fallback: if predict returns logits or probs, adapt as needed\n",
    "            p_preds[\"cnn1d\"] = np.clip(mdl.predict(X_test), 0, 1).astype(float).ravel()\n",
    "        labels[\"cnn1d\"] = (\"CNN-1D\", average_precision_score(y_test, p_preds[\"cnn1d\"]))\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "for k, p in p_preds.items():\n",
    "    P, R, AP = pr_arrays(y_test, p)\n",
    "    plt.plot(R, P, linewidth=1, label=f\"{k} (AP={AP:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curves\")\n",
    "plt.grid(True, linewidth=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3081fa86",
   "metadata": {},
   "source": [
    "## 18. Threshold vs F1 (Champion vs 1D-CNN)\n",
    "Show \\( $F1(\\tau)$ \\) to justify the **operational threshold** choice for BO vs 1D-CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Threshold vs F1 curve comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "taus = np.linspace(0.01, 0.99, 50)\n",
    "def f1_curve(y_true, p_hat, ts):\n",
    "    vals = []\n",
    "    for t in ts:\n",
    "        vals.append(f1_score(y_true, (p_hat >= t).astype(int), zero_division=0))\n",
    "    return np.array(vals)\n",
    "\n",
    "curves = {}\n",
    "# champion: prefer bo_enhanced then bo_lgb then random_search then manual_grid then baseline\n",
    "pref = None\n",
    "for s in [\"bo_enhanced\",\"bo_lgb\",\"random_search\",\"manual_grid\"]:\n",
    "    if s in p_preds:\n",
    "        pref = s\n",
    "        break\n",
    "if pref:\n",
    "    curves[pref] = f1_curve(y_test, p_preds[pref], taus)\n",
    "else:\n",
    "    curves[\"baseline\"] = f1_curve(y_test, p_preds[\"baseline\"], taus)\n",
    "\n",
    "# cnn if available\n",
    "if \"cnn1d\" in p_preds:\n",
    "    curves[\"cnn1d\"] = f1_curve(y_test, p_preds[\"cnn1d\"], taus)\n",
    "\n",
    "plt.figure()\n",
    "for name, vals in curves.items():\n",
    "    plt.plot(taus, vals, linewidth=1, label=name)\n",
    "plt.xlabel(\"τ\")\n",
    "plt.ylabel(\"F1\")\n",
    "plt.title(\"F1 vs τ (threshold sweep)\")\n",
    "plt.grid(True, linewidth=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27023f1",
   "metadata": {},
   "source": [
    "## 19. Feature Importance (Champion LightGBM)\n",
    "Bar chart of **gain** importances from the champion LightGBM to justify interpretability and selected hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) Feature importance for champion LightGBM (if available)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "champ_model = None\n",
    "for s in [\"bo_enhanced\",\"bo_lgb\",\"random_search\",\"manual_grid\"]:\n",
    "    p = Path(\"staging\")/s/\"model.joblib\"\n",
    "    if p.exists():\n",
    "        import joblib\n",
    "        champ_model = joblib.load(p)\n",
    "        champ_stage = s\n",
    "        break\n",
    "\n",
    "if champ_model is not None and hasattr(champ_model, \"booster_\"):\n",
    "    booster = champ_model.booster_\n",
    "    try:\n",
    "        imp = booster.feature_importance(importance_type=\"gain\")\n",
    "        names = booster.feature_name()\n",
    "        imp_df = pd.DataFrame({\"feature\": names, \"gain\": imp}).sort_values(\"gain\", ascending=False).head(20)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.barh(range(len(imp_df)), imp_df[\"gain\"].values)\n",
    "        plt.yticks(range(len(imp_df)), imp_df[\"feature\"].values)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.xlabel(\"Gain Importance\")\n",
    "        plt.title(f\"Top-20 Feature Importances ({champ_stage})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Could not extract LightGBM importances:\", e)\n",
    "else:\n",
    "    print(\"Champion LightGBM model not found or missing booster_.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48702ff0",
   "metadata": {},
   "source": [
    "## 20. 1D-CNN Benchmark Loader\n",
    "If you trained a 1D-CNN outside this notebook, drop its test predictions or model into `staging/cnn1d/`:\n",
    "- `model.joblib` with `predict_proba(X_test)[:,1]` or `predict(X_test)` in \\([0,1]\\),\n",
    "- or a `preds.npy` of probabilities.\n",
    "Then run this cell to compute **AUPRC/F1/τ** and stage a manifest for fair comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) 1D-CNN benchmark loader → stage manifest\n",
    "\n",
    "cnn_dir = stage_dir(\"cnn1d\")\n",
    "cnn_model = cnn_dir / \"model.joblib\"\n",
    "cnn_preds = cnn_dir / \"preds.npy\"\n",
    "\n",
    "p_cnn = None\n",
    "if cnn_preds.exists():\n",
    "    p_cnn = np.load(cnn_preds)\n",
    "elif cnn_model.exists():\n",
    "    import joblib\n",
    "    mdl = joblib.load(cnn_model)\n",
    "    try:\n",
    "        p_cnn = mdl.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        p_cnn = np.clip(mdl.predict(X_test), 0, 1).astype(float).ravel()\n",
    "\n",
    "if p_cnn is not None:\n",
    "    auprc_cnn = average_precision_score(y_test, p_cnn)\n",
    "    f1_cnn, tau_cnn = sweep_f1(y_test, p_cnn)\n",
    "    tpr_cnn, tnr_cnn = tpr_tnr_at_tau(y_test, p_cnn, tau_cnn)\n",
    "    save_stage(\"cnn1d\", {\n",
    "        \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n",
    "        \"stage\": \"cnn1d\",\n",
    "        \"model_name\": \"CNN1D\",\n",
    "        \"dataset\": \"archive/Payload_data_UNSW.csv\",\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"metrics\": {\"AUPRC\": float(auprc_cnn), \"F1\": float(f1_cnn), \"tau\": float(tau_cnn),\n",
    "                    \"TPR\": float(tpr_cnn), \"TNR\": float(tnr_cnn)}\n",
    "    })\n",
    "    print(\"CNN1D staged | AUPRC={:.4f} F1@τ={:.4f} τ={:.2f}\".format(auprc_cnn, f1_cnn, tau_cnn))\n",
    "else:\n",
    "    print(\"No CNN predictions/model found in staging/cnn1d/. Provide model.joblib or preds.npy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc4f478",
   "metadata": {},
   "source": [
    "## 21. Champion-Driven Auto-Wire (optional)\n",
    "Auto-wire **the current champion** (best AUPRC → F1) into `model_card.md` and `README.md`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21) Champion-driven auto-wire\n",
    "# Reuse previous champion selection if dfm exists; else recompute here\n",
    "if 'dfm' not in globals() or dfm.empty:\n",
    "    metrics = {}\n",
    "    for s in [\"manual_grid\", \"random_search\", \"bo_lgb\", \"bo_enhanced\", \"cnn1d\"]:\n",
    "        m = load_stage(s)\n",
    "        if m and \"metrics\" in m:\n",
    "            metrics[s] = m[\"metrics\"]\n",
    "    dfm = pd.DataFrame(metrics).T if metrics else pd.DataFrame()\n",
    "\n",
    "if not dfm.empty:\n",
    "    champ_name = dfm.sort_values([\"AUPRC\", \"F1\"], ascending=[False, False]).index[0]\n",
    "    m = load_stage(champ_name)[\"metrics\"]\n",
    "    auprc = m.get(\"AUPRC\") or m.get(\"auprc\")\n",
    "    f1 = m.get(\"F1\") or m.get(\"f1\")\n",
    "    tau = m.get(\"tau\")\n",
    "    block = (\n",
    "        \"<!--METRICS_START-->\\n\"\n",
    "        f\"Stage: {champ_name}\\n\"\n",
    "        f\"AUPRC: {auprc:.4f}\\n\"\n",
    "        f\"F1: {f1:.4f}\\n\"\n",
    "        f\"τ: {tau:.2f}\\n\"\n",
    "        \"<!--METRICS_END-->\\n\"\n",
    "    )\n",
    "    for doc in [\"model_card.md\", \"README.md\"]:\n",
    "        p = Path(doc)\n",
    "        if not p.exists(): \n",
    "            continue\n",
    "        txt = p.read_text()\n",
    "        if \"<!--METRICS_START-->\" in txt and \"<!--METRICS_END-->\" in txt:\n",
    "            pre, _, rest = txt.partition(\"<!--METRICS_START-->\")\n",
    "            _, _, post = rest.partition(\"<!--METRICS_END-->\")\n",
    "            txt = pre + block + post\n",
    "        else:\n",
    "            if not txt.endswith(\"\\n\"):\n",
    "                txt += \"\\n\"\n",
    "            txt += \"\\n\" + block\n",
    "        p.write_text(txt)\n",
    "    print(\"Docs updated with champion metrics:\", champ_name)\n",
    "else:\n",
    "    print(\"No metrics available to auto-wire champion.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
