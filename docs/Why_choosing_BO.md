# Why Bayesian Optimization (BO) Was Chosen for This Project

In this Intrusion Detection System (IDS) project, Bayesian Optimization (BO) is used to tune the hyperparameters of the LightGBM model for binary intrusion detection (benign vs. non-benign traffic) on tabular data, such as flow statistics or payload histograms from datasets like UNSW-NB15. BO is particularly suitable because it efficiently optimizes the model's performance metric (PR-AUC) in a resource-constrained environment, where each evaluation (e.g., 5-fold cross-validation) is computationally expensive. The notebook demonstrates BO's application in sections 5 (standard BO) and 16 (extended BO with class weighting), achieving high PR-AUC (\~0.95+) with only 40 iterations.

BO's key advantages align with the project's goals of reproducibility, efficiency, and handling imbalanced data (Module 10: Optimization). It uses a probabilistic surrogate model (e.g., Gaussian Process) to predict PR-AUC for untried hyperparameter settings, intelligently selecting the next trial via an acquisition function that balances exploration (uncertain regions) and exploitation (promising regions). This is ideal for the high-dimensional search space of LightGBM (7–8 hyperparameters like `num_leaves`, `learning_rate`, and `scale_pos_weight` in section 16), reducing trials compared to exhaustive methods while converging to strong hyperparameters, as visualized in the response surface plot (section 7).

In contrast, other hyperparameter tuning methods (e.g., grid search in section 5) or alternative models (e.g., 1D-CNN baseline in sections 10–14) were considered but not chosen as the primary approach due to their limitations in efficiency, interpretability, and performance on tabular data. However, extending BO to tune the 1D-CNN's hyperparameters (e.g., number of filters, dropout rate, learning rate) could potentially improve its performance, making it a viable alternative. Below, we explore this extension and compare it to the current approach.

## Compare Bayesian Optimization to Neural Networks

Applying BO to the 1D-CNN in the IDS project involves defining a search space for its hyperparameters, such as the number of convolutional filters (e.g., 32–128), kernel size (e.g., 3–7), dropout rate (e.g., 0.1–0.5), learning rate (e.g., 1e-4–1e-2, log-uniform), and dense layer units (e.g., 32–256). The notebook's 1D-CNN (section 11) currently uses fixed hyperparameters (32/64 filters, kernel size=3, dropout=0.2, learning rate=1e-3), limiting its optimization potential. Using `BayesSearchCV` or a custom BO loop (e.g., with `skopt`'s `gp_minimize`), we could optimize these parameters over 40 iterations, matching the LightGBM BO setup, to maximize PR-AUC on 5-fold stratified CV.

### Benefits of BO for 1D-CNN

1. **Efficiency in Expensive Evaluations**: Training a CNN is computationally intensive (60 epochs with GPU in section 12). BO minimizes the number of training runs by modeling the PR-AUC landscape, potentially finding better hyperparameters than the fixed setup in fewer trials.
2. **Handling Complex Search Spaces**: The 1D-CNN has a high-dimensional, non-linear hyperparameter space (e.g., architectural and optimization parameters). BO’s surrogate model can navigate this complexity, as it does for LightGBM’s 7–8 parameters.
3. **Improved Performance**: The fixed 1D-CNN achieves \~0.90 PR-AUC (section 13), lower than LightGBM’s \~0.95+ (section 15). BO could optimize parameters like dropout or learning rate to reduce overfitting and improve PR-AUC, potentially closing the gap with LightGBM.
4. **Robustness to Imbalance**: By optimizing class weights or loss weighting (similar to `scale_pos_weight` in section 16), BO could better handle the dataset’s imbalance (\~73.6% malicious, section 18), improving TPR and TNR for SOC requirements.

### Challenges of BO for 1D-CNN

1. **Increased Computational Cost**: Each CNN evaluation requires training for multiple epochs (e.g., 60 in section 12), significantly slower than LightGBM’s CV iterations. Even with 40 BO trials, this could be infeasible without substantial GPU resources.
2. **Complex Search Space**: Architectural hyperparameters (e.g., number of layers) are discrete and harder to model than continuous parameters like `learning_rate`. BO may struggle with mixed integer-continuous spaces, requiring careful prior definitions.
3. **Interpretability**: Even with BO, the CNN remains less interpretable than LightGBM, lacking feature importance or clear threshold tuning (section 8), which is critical for SOC integration.
4. **Overfitting Risk**: NNs are prone to overfitting on tabular data, especially with smaller datasets. BO may select high-capacity architectures (e.g., more filters) that overfit, requiring additional regularization tuning.

## Comparison of Pros and Cons of Other Models/Methods

Below is an updated comparison table including BO-tuned 1D-CNN, evaluating suitability for the IDS project based on computational cost, handling imbalance, interpretability, and performance on tabular data. The table retains the original methods (BO for LightGBM, grid search, random search, fixed 1D-CNN) and adds BO-tuned 1D-CNN.

| Method/Model | Pros | Cons | Why Not Chosen as Primary |
| --- | --- | --- | --- |
| **Bayesian Optimization (BO) for LightGBM** (Primary in Notebook, Sections 6, 16) | \- **Efficient**: Uses surrogate model to minimize expensive evaluations (e.g., 40 trials vs. thousands in grid search).<br>- **Handles high-dimensional spaces**: Balances exploration/exploitation for optimal hyperparameters (Module 10).<br>- **Robust to noise**: Smooths variable CV scores on imbalanced data.<br>- **High performance**: Achieves \~0.95+ PR-AUC in extended BO (section 16).<br>- **Interpretable**: LightGBM provides feature importance and supports threshold tuning for SOC (section 9). | \- Requires setup (e.g., defining log-uniform priors for `learning_rate`).<br>- Black-box: Less intuitive than manual tuning for beginners. | **Chosen**: Best balance of efficiency, performance, and interpretability for tuning LightGBM on tabular, imbalanced data. |
| **Grid Search** (Manual Example in Section 5) | \- **Exhaustive**: Tests all combinations, ensuring no optima are missed in small grids.<br>- **Simple**: Easy to implement and understand.<br>- **Reproducible**: Fixed grid (e.g., 3x3 for `num_leaves` and `learning_rate`). | \- **Inefficient**: Computationally expensive for high-dimensional spaces (e.g., 7 parameters would require 10^7+ trials).<br>- **Scales poorly**: Small grid yields lower PR-AUC (\~0.85) than BO (\~0.95+).<br>- **No intelligence**: Doesn’t adapt based on prior results. | Not primary: Too slow for full search space; used only for demonstration. BO finds better parameters with fewer trials. |
| **Random Search** (Alternative to Grid Search) | \- **Faster than grid**: Samples randomly, better for high dimensions.<br>- **Flexible**: Can run for a fixed budget (e.g., 40 trials like BO).<br>- **Handles imbalance**: Can optimize PR-AUC directly. | \- **Less efficient than BO**: No learning from prior trials, may waste evaluations on poor regions.<br>- **Variable results**: Depends on luck; lower average PR-AUC than BO.<br>- **No surrogate**: Doesn’t model the optimization landscape. | Not primary: Lacks BO’s principled approach; random search could replace `BayesSearchCV` but would likely yield lower PR-AUC. |
| **Neural Networks (1D-CNN, Fixed Parameters)** (Baseline in Sections 10–14) | \- **Powerful for sequences**: Learns local patterns in payload histograms (reshaped to (N, L, 1) in section 10).<br>- **Flexible**: Handles raw features without manual engineering.<br>- **High capacity**: Achieves \~0.90 PR-AUC with class weights (section 13). | \- **Computationally heavy**: Requires more resources (60 epochs, GPU preferred) than LightGBM.<br>- **Less effective on tabular data**: Underperforms LightGBM (section 15).<br>- **Harder to interpret**: Lacks feature importance; tuning is complex and not automated.<br>- **Overfitting risk**: Needs early stopping and dropout, sensitive to imbalance and data size. | Not primary: LightGBM with BO is faster, more interpretable, and better for tabular IDS data (SOC metrics, section 9). CNN is a baseline only. |
| **Bayesian Optimization for 1D-CNN** (Proposed Extension) | \- **Efficient tuning**: Reduces the number of CNN training runs (e.g., 40 trials vs. grid search).<br>- **Improves performance**: Could optimize filters, dropout, and learning rate to improve PR-AUC beyond \~0.90.<br>- **Handles imbalance**: Can tune class weights or loss weighting, improving TPR/TNR.<br>- **Flexible**: Adapts to complex NN architectures via BO’s surrogate model. | \- **Very computationally expensive**: Each trial requires training for multiple epochs, far slower than LightGBM (hours vs. minutes).<br>- **Complex search space**: Architectural parameters (e.g., layers) are hard to model, requiring careful priors.<br>- **Less interpretable**: CNNs lack LightGBM’s feature importance, limiting SOC integration.<br>- **Overfitting risk**: High-capacity models may overfit tabular data, even with BO.<br>- **Implementation complexity**: Requires wrapping CNN in a scikit-learn estimator for `BayesSearchCV`. | Not primary: High computational cost and lower interpretability make it less practical than BO-tuned LightGBM. Likely to underperform on tabular data, even with optimization. |

## Conclusion

BO is chosen as the primary method for tuning LightGBM’s hyperparameters in this IDS project due to its efficiency, ability to handle high-dimensional spaces, robustness to noisy and imbalanced data, and alignment with SOC requirements for interpretability (e.g., TPR/TNR in section 9). Extending BO to the 1D-CNN could improve its performance by optimizing hyperparameters like filters and dropout, potentially closing the PR-AUC gap with LightGBM (\~0.90 vs. \~0.95+). However, the CNN’s high computational cost, complex search space, and lower interpretability make it less suitable as the primary model, even with BO. Grid and random search are less adaptive and efficient than BO, while the fixed 1D-CNN underperforms on tabular data. The notebook’s comparison (section 15) and optimization results (section 17) confirm BO-tuned LightGBM’s superiority for this project. If resources permit, implementing BO for the 1D-CNN could serve as a stronger baseline, but LightGBM remains the optimal choice for deployment.
